{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f469aef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Opening the csv file without any parsing (UTF-8 encoding by default)\n",
    "with open(\"training_data_lowercase.csv\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "rows = [line.split(\"\\t\", 1) for line in lines]\n",
    "data = pd.DataFrame(rows, columns=[\"label\", \"text\"])\n",
    "\n",
    "data[\"label\"] = data[\"label\"].astype(str).str.replace(\"\\ufeff\", \"\", regex=False)\n",
    "data[\"label\"] = pd.to_numeric(data[\"label\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "test_size = int(len(data)* 0.1)\n",
    "test_indices = random.sample(range(0, len(data)), test_size)\n",
    "\n",
    "train_data = data.drop(index=test_indices)\n",
    "test_data = data.iloc[test_indices]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(2, 1)) \n",
    "plt.xticks([0, 1])\n",
    "data['label'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c573ac",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4881d075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>donald trump sends out embarrassing new year‚s eve message; this is disturbing</td>\n",
       "      <td>donald trump sends out embarrassing new year eve message this is disturbing</td>\n",
       "      <td>[donald, trump, sends, out, embarrassing, new, year, eve, message, this, is, disturbing]</td>\n",
       "      <td>donald trump sends out embarrassing new year eve message this is disturbing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>drunk bragging trump staffer started russian collusion investigation</td>\n",
       "      <td>drunk bragging trump staffer started russian collusion investigation</td>\n",
       "      <td>[drunk, bragging, trump, staffer, started, russian, collusion, investigation]</td>\n",
       "      <td>drunk bragging trump staffer started russian collusion investigation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "\n",
       "                                                                             text  \\\n",
       "0  donald trump sends out embarrassing new year‚s eve message; this is disturbing   \n",
       "1            drunk bragging trump staffer started russian collusion investigation   \n",
       "\n",
       "                                                                    clean_text  \\\n",
       "0  donald trump sends out embarrassing new year eve message this is disturbing   \n",
       "1         drunk bragging trump staffer started russian collusion investigation   \n",
       "\n",
       "                                                                                     tokens  \\\n",
       "0  [donald, trump, sends, out, embarrassing, new, year, eve, message, this, is, disturbing]   \n",
       "1             [drunk, bragging, trump, staffer, started, russian, collusion, investigation]   \n",
       "\n",
       "                                                             preprocessed_text  \n",
       "0  donald trump sends out embarrassing new year eve message this is disturbing  \n",
       "1         drunk bragging trump staffer started russian collusion investigation  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(language = 'english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def clean_html_regex(text: str) -> str:\n",
    "    # Remove <script>...</script> and <style>...</style> blocks (inline JS/CSS)\n",
    "    text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    # Remove HTML comments <!-- ... -->. # (must be done before removing other tags since comments may contain '>')\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove any remaining HTML tags like <div>, <p>, <a href=\"...\"> etc.\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # Optional: normalize spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^A-Za-z\\s]', ' ', text)             # keep letters, spaces, and !\n",
    "    text = re.sub(r'\\b\\w\\b', '', text, flags=re.UNICODE)  # remove single-letter words\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()              # collapse multiple spaces\n",
    "    text = re.sub(r\"^b'(.*)'$\", r\"\\1\", text)              # remove leading 'b\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    text = clean_html_regex(text)\n",
    "    text = clean_text(text)\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # tokens = [\n",
    "    #     t\n",
    "    #     for t in tokens\n",
    "    #     if t not in stop_words and t not in string.punctuation\n",
    "    # ]\n",
    "\n",
    "    # tokens = [\n",
    "    #     stemmer.stem(t)\n",
    "    #     for t in tokens\n",
    "    # ]\n",
    "\n",
    "    clean_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    #return text\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "data_train = train_data.copy()\n",
    "data_test = test_data.copy()\n",
    "\n",
    "data_train['clean_text'] = data_train['text'].apply(clean_html_regex).apply(clean_text)\n",
    "data_test['clean_text'] = data_test['text'].apply(clean_html_regex).apply(clean_text)\n",
    "\n",
    "data_train['tokens'] = data_train['text'].apply(preprocess_text)\n",
    "data_test['tokens'] = data_test['text'].apply(preprocess_text)\n",
    "\n",
    "data_train['preprocessed_text'] = data_train['tokens'].apply(lambda x: ' '.join(x))\n",
    "data_test['preprocessed_text'] = data_test['tokens'].apply (lambda x: ' '.join(x))\n",
    "\n",
    "data_train.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da38835",
   "metadata": {},
   "source": [
    "### Bow + Naive Bayes and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23416a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Naive Bayes Accuracy Score: 0.9452415812591508\n",
      "Bow Naive Bayes F1 Score: 0.9452253557495881\n",
      "Bow Logistic Regression Accuracy Score: 0.9443631039531479\n",
      "Bow Logistic Regression F1 Score: 0.9443749900001187\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bow_vect = CountVectorizer(max_features=15000, ngram_range=(1,1))\n",
    "\n",
    "# fit creates one entry for each different word seen\n",
    "X_bow_train = bow_vect.fit_transform(data_train['preprocessed_text']).toarray()\n",
    "train_bow_df = pd.DataFrame(X_bow_train,columns=bow_vect.get_feature_names_out())\n",
    "\n",
    "X_bow_test = bow_vect.transform(data_test['preprocessed_text']).toarray()\n",
    "test_bow_df = pd.DataFrame(X_bow_test,columns=bow_vect.get_feature_names_out())\n",
    "#test_bow_df.head(2)\n",
    "\n",
    "X_train = train_bow_df.copy()\n",
    "y_train = data_train['label'].copy()\n",
    "\n",
    "X_test = test_bow_df.copy()\n",
    "y_test = data_test['label'].copy()\n",
    "\n",
    "nb_model_bow = MultinomialNB()\n",
    "nb_model_bow.fit(X_train, y_train)\n",
    "y_pred = nb_model_bow.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "print(\"BoW Naive Bayes Accuracy Score:\", acc)\n",
    "print(\"Bow Naive Bayes F1 Score:\", f1)\n",
    "\n",
    "lr_model_bow = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "lr_model_bow.fit(X_train, y_train)\n",
    "y_pred = lr_model_bow.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "print(\"Bow Logistic Regression Accuracy Score:\", acc)\n",
    "print(\"Bow Logistic Regression F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9e7bf",
   "metadata": {},
   "source": [
    "### TF-IDF + Naive Bayes and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce7acafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Naive Bayes Accuracy Score: 0.939385065885798\n",
      "TF-IDF Naive Bayes F1 Score: 0.9393779670756276\n",
      "TF-IDF Logistic Regression Accuracy Score: 0.9443631039531479\n",
      "TF-IDF Logistic Regression F1 Score: 0.9443754765502436\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#tfidf_vectorizer = TfidfVectorizer(max_features=10000, min_df = 10, max_df = 0.5, ngram_range=(1,1))\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=20000,  # Keep top 5000 most important words\n",
    "    ngram_range=(1, 3),  # Use unigrams and bigrams\n",
    "    min_df=2,  # Ignore words appearing in fewer than 2 documents\n",
    "    max_df=0.8  # Ignore words appearing in more than 80% of documents\n",
    ")\n",
    "\n",
    "\n",
    "X_tfidf_train = tfidf_vectorizer.fit_transform(data_train['preprocessed_text'])\n",
    "train_tfidf_df = pd.DataFrame(X_tfidf_train.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "X_tfidf_test = tfidf_vectorizer.transform(data_test['preprocessed_text'])\n",
    "test_tfidf_df = pd.DataFrame(X_tfidf_test.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "X_train = train_tfidf_df.copy()\n",
    "y_train = data_train['label'].copy()\n",
    "\n",
    "X_test = test_tfidf_df.copy()\n",
    "y_test = data_test['label'].copy()\n",
    "\n",
    "\n",
    "nb_model_tfidf = MultinomialNB()\n",
    "nb_model_tfidf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = nb_model_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "print(\"TF-IDF Naive Bayes Accuracy Score:\", acc)\n",
    "print(\"TF-IDF Naive Bayes F1 Score:\", f1)\n",
    "\n",
    "lr_model_tfidf = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "lr_model_tfidf.fit(X_train, y_train)\n",
    "y_pred = lr_model_tfidf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "print(\"TF-IDF Logistic Regression Accuracy Score:\", acc)\n",
    "print(\"TF-IDF Logistic Regression F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664fb84e",
   "metadata": {},
   "source": [
    "## Sentence Tranformer + Random Forest and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dac1ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "#embedder = SentenceTransformer(\"all‑mpnet‑base‑v2\")\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "X_train_mpnet = embedder.encode(data_train[\"clean_text\"].tolist())\n",
    "X_test_mpnet  = embedder.encode(data_test[\"clean_text\"].tolist())\n",
    "\n",
    "y_train = data_train[\"label\"]\n",
    "y_test = data_test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04b9a26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.93      0.92      1764\n",
      "         1.0       0.92      0.91      0.92      1651\n",
      "\n",
      "    accuracy                           0.92      3415\n",
      "   macro avg       0.92      0.92      0.92      3415\n",
      "weighted avg       0.92      0.92      0.92      3415\n",
      "\n",
      "[[1637  127]\n",
      " [ 146 1505]]\n",
      "Accuracy:  0.9200585651537335\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,        # number of trees\n",
    "    max_depth=None,          # let it grow fully\n",
    "    random_state=42,\n",
    "    n_jobs=-1                # use all CPU cores\n",
    ")\n",
    "rf.fit(X_train_mpnet, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test_mpnet)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4411a93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.96      0.95      1764\n",
      "         1.0       0.95      0.94      0.94      1651\n",
      "\n",
      "    accuracy                           0.95      3415\n",
      "   macro avg       0.95      0.95      0.95      3415\n",
      "weighted avg       0.95      0.95      0.95      3415\n",
      "\n",
      "[[1685   79]\n",
      " [ 105 1546]]\n",
      "Accuracy:  0.9461200585651537\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "clf = xgb.XGBClassifier(\n",
    "    n_estimators= 500,\n",
    "    learning_rate=0.15,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha= 2,\n",
    "    reg_lambda = 1,\n",
    "    eval_metric=\"logloss\",\n",
    "    use_label_encoder=False,\n",
    "    tree_method=\"hist\",  # for speed\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clf.fit(X_train_mpnet, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_mpnet)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f388d7c",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bfc96d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"testing_data_lowercase_nolabels.csv\", encoding=\"utf-8\") as f:\n",
    "    final_lines = f.read().splitlines()\n",
    "\n",
    "final_rows = [line.split(\"\\t\", 1) for line in final_lines]\n",
    "final_data = pd.DataFrame(final_rows, columns=[\"label\", \"text\"])\n",
    "\n",
    "final_data['tokens'] = final_data['text'].apply(preprocess_text)\n",
    "final_data['preprocessed_text'] = final_data['tokens'].apply (lambda x: ' '.join(x))\n",
    "\n",
    "#bow_vect = CountVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "X_final_data = bow_vect.transform(final_data['preprocessed_text']).toarray()\n",
    "final_bow_df = pd.DataFrame(X_final_data, columns = bow_vect.get_feature_names_out())\n",
    "\n",
    "\n",
    "y_pred_final = lr_model_bow.predict(final_bow_df)\n",
    "final_data['label'] = y_pred_final\n",
    "\n",
    "plt.figure(figsize=(2, 1)) \n",
    "plt.xticks([0, 1])\n",
    "final_data['label'].hist()\n",
    "plt.show()\n",
    "\n",
    "final_data= final_data[['label','text']]\n",
    "\n",
    "final_data.to_csv(\"testing_data_lowercase_results.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news-articles-classifier-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
